{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks for Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#global imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import PatchesDataLoader, PatchesSKLearner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data_path = '/home/jacky/ws/patches/data/'\n",
    "data_path = '/mnt/wd_ssd/projects/deep_patches/data/patches_06_11_16'\n",
    "pdl = PatchesDataLoader(0.25, data_path, [i for i in range(1)], by_objs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(types, labels):\n",
    "    result = np.zeros((labels.shape[0], len(types)))\n",
    "    for i in range(labels.shape[0]):\n",
    "        result[i][types[labels[i]]] = 1\n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_tr_1h = one_hot([0, 1], pdl.labels['fc']['tr'])\n",
    "fc_t_1h = one_hot([0, 1], pdl.labels['fc']['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pfc_10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-83e4503b72cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpfc_10_tr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pfc_10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpfc_10_t\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pfc_10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m't'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pfc_10'"
     ]
    }
   ],
   "source": [
    "pfc_10_tr = pdl.labels['pfc_10']['tr'].reshape(-1, 1)\n",
    "pfc_10_t = pdl.labels['pfc_10']['t'].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cropped_X(pdl, dim):\n",
    "    \n",
    "    def crop(img, dim):\n",
    "        big_dim = int(np.sqrt(img.shape[0]))\n",
    "        img = img.reshape(big_dim, big_dim)\n",
    "        mid = big_dim // 2\n",
    "        delta = dim // 2\n",
    "\n",
    "        small_img = img[mid - delta : mid + delta + 1, mid - delta : mid + delta + 1]\n",
    "\n",
    "        return small_img.reshape(dim*dim)\n",
    "\n",
    "    cropped_w1 = np.array([crop(pdl._raw_data['w1_proj'][i,:], 3) for i in range(len(pdl._raw_data['w1_proj']))])\n",
    "    cropped_w2 = np.array([crop(pdl._raw_data['w2_proj'][i,:], 3) for i in range(len(pdl._raw_data['w2_proj']))])\n",
    "\n",
    "    cropped_w1_tr, cropped_w1_t, _ = pdl.split_train_test(cropped_w1, 0.25, pdl.indices)\n",
    "    cropped_w2_tr, cropped_w2_t, _ = pdl.split_train_test(cropped_w1, 0.25, pdl.indices)\n",
    "\n",
    "    X = pdl.get_partial_train_data(('moment_arms', 'patch_ori'))\n",
    "\n",
    "    X['tr'] = np.c_[X['tr'], cropped_w1_tr, cropped_w2_tr]\n",
    "    X['t'] = np.c_[X['t'], cropped_w1_t, cropped_w2_t]\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = get_cropped_X(pdl, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = skflow.TensorFlowDNNClassifier(hidden_units=[50, 50, 50], n_classes=2, batch_size=batch_size, steps=5000,\n",
    "                                    optimizer=\"Adam\", learning_rate=0.001, dropout=0.25, continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #4, avg. train loss: 0.67778\n",
      "Step #200, epoch #9, avg. train loss: 0.62479\n",
      "Step #300, epoch #13, avg. train loss: 0.60075\n",
      "Step #400, epoch #18, avg. train loss: 0.57387\n",
      "Step #500, epoch #22, avg. train loss: 0.55217\n",
      "Step #600, epoch #27, avg. train loss: 0.54057\n",
      "Step #700, epoch #31, avg. train loss: 0.52769\n",
      "Step #800, epoch #36, avg. train loss: 0.51661\n",
      "Step #900, epoch #40, avg. train loss: 0.51055\n",
      "Step #1000, epoch #45, avg. train loss: 0.50031\n",
      "Step #1100, epoch #50, avg. train loss: 0.49292\n",
      "Step #1200, epoch #54, avg. train loss: 0.48682\n",
      "Step #1300, epoch #59, avg. train loss: 0.47828\n",
      "Step #1400, epoch #63, avg. train loss: 0.47869\n",
      "Step #1500, epoch #68, avg. train loss: 0.47181\n",
      "Step #1600, epoch #72, avg. train loss: 0.46332\n",
      "Step #1700, epoch #77, avg. train loss: 0.46078\n",
      "Step #1800, epoch #81, avg. train loss: 0.45502\n",
      "Step #1900, epoch #86, avg. train loss: 0.44796\n",
      "Step #2000, epoch #90, avg. train loss: 0.44984\n",
      "Step #2100, epoch #95, avg. train loss: 0.43965\n",
      "Step #2200, epoch #100, avg. train loss: 0.43582\n",
      "Step #2300, epoch #104, avg. train loss: 0.43507\n",
      "Step #2400, epoch #109, avg. train loss: 0.42792\n",
      "Step #2500, epoch #113, avg. train loss: 0.42448\n",
      "Step #2600, epoch #118, avg. train loss: 0.42350\n",
      "Step #2700, epoch #122, avg. train loss: 0.41810\n",
      "Step #2800, epoch #127, avg. train loss: 0.41801\n",
      "Step #2900, epoch #131, avg. train loss: 0.41182\n",
      "Step #3000, epoch #136, avg. train loss: 0.40847\n",
      "Step #3100, epoch #140, avg. train loss: 0.41182\n",
      "Step #3200, epoch #145, avg. train loss: 0.40808\n",
      "Step #3300, epoch #150, avg. train loss: 0.40567\n",
      "Step #3400, epoch #154, avg. train loss: 0.39938\n",
      "Step #3500, epoch #159, avg. train loss: 0.39651\n",
      "Step #3600, epoch #163, avg. train loss: 0.40560\n",
      "Step #3700, epoch #168, avg. train loss: 0.39698\n",
      "Step #3800, epoch #172, avg. train loss: 0.39354\n",
      "Step #3900, epoch #177, avg. train loss: 0.39121\n",
      "Step #4000, epoch #181, avg. train loss: 0.38608\n",
      "Step #4100, epoch #186, avg. train loss: 0.39266\n",
      "Step #4200, epoch #190, avg. train loss: 0.38747\n",
      "Step #4300, epoch #195, avg. train loss: 0.38295\n",
      "Step #4400, epoch #200, avg. train loss: 0.38142\n",
      "Step #4500, epoch #204, avg. train loss: 0.38585\n",
      "Step #4600, epoch #209, avg. train loss: 0.37890\n",
      "Step #4700, epoch #213, avg. train loss: 0.37051\n",
      "Step #4800, epoch #218, avg. train loss: 0.38091\n",
      "Step #4900, epoch #222, avg. train loss: 0.37718\n",
      "Step #5000, epoch #227, avg. train loss: 0.37169\n",
      "took 19.6682641506s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "clf.fit(X['tr'], pdl.labels['fc']['tr'])\n",
    "end = time.time()\n",
    "print 'took {0}s'.format(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Classification train accuracy 0.856011183597, test accuracy 0.758041958042\n"
     ]
    }
   ],
   "source": [
    "PatchesSKLearner.print_accuracy(clf, X['tr'], pdl.labels['fc']['tr'], X['t'], pdl.labels['fc']['t'], \"DNN Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search CV on Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LoopTimingForecaster:\n",
    "    \n",
    "    def __init__(self, n):\n",
    "        self._n = n\n",
    "        self._i = 0\n",
    "        self._start = None\n",
    "        self._temp_start = None\n",
    "        self._end = None\n",
    "        \n",
    "        self._times = []\n",
    "        \n",
    "    def _check_end(self):\n",
    "        if self._end is not None:\n",
    "            raise Exception(\"Can't perform this operation because end has already been called\")\n",
    "                \n",
    "    def report_forecast(self):\n",
    "        self._check_end()\n",
    "        if len(self._times) == 0:\n",
    "            print \"No data on timing yet. Can't perform forecast\"\n",
    "        else:\n",
    "            remaining_steps = self._n - self._i\n",
    "            mean_time = sum(self._times) / len(self._times)\n",
    "            remaining_time = mean_time * remaining_steps\n",
    "            \n",
    "            msg = \"Remaining time: {0}, steps: {1}. Time per step: {2}\".format(timedelta(seconds=remaining_time),\n",
    "                                                                               remaining_steps, timedelta(seconds=mean_time))\n",
    "            print msg\n",
    "            \n",
    "    def record_loop_start(self):\n",
    "        cur_time = time.time()\n",
    "        self._check_end()\n",
    "        if self._start is None:\n",
    "            self._start = cur_time\n",
    "            print \"Starting loop timer\"\n",
    "        self._temp_start = cur_time\n",
    "    \n",
    "    def record_loop_end(self):\n",
    "        cur_time = time.time()\n",
    "        self._check_end()\n",
    "        \n",
    "        if self._temp_start is None:\n",
    "            raise Exception(\"Can't end loop timer because it hasn't been started!\")\n",
    "        \n",
    "        self._times.append(cur_time - self._temp_start)\n",
    "        self._temp_start = None\n",
    "        self._i += 1\n",
    "        \n",
    "    def record_end(self):\n",
    "        self._end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SKFlowGridSearchCV:\n",
    "    \n",
    "    def __init__(self, estimator_instantiator, params, cv=0.25, random_state=None):\n",
    "        self._estimator_instantiator = estimator_instantiator\n",
    "        self._cv = cv\n",
    "        self._random_state = random_state\n",
    "        self._best_score = None\n",
    "        self._best_params = None\n",
    "        self._best_estimator = None\n",
    "        self._scores = None\n",
    "        self._params = params\n",
    "        self._params_grid = None\n",
    "        self._generate_params_grid()\n",
    "        \n",
    "    def _generate_params_grid(self):\n",
    "        self._params_grid = []\n",
    "        keys = self._params.keys()\n",
    "        options = self._params.values()\n",
    "        options_combos = itertools.product(*options)\n",
    "\n",
    "        for options_combo in options_combos:\n",
    "            param = {}\n",
    "            for i in range(len(keys)):\n",
    "                param[keys[i]] = options_combo[i]\n",
    "\n",
    "            self._params_grid.append(param)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._scores = []\n",
    "        self._best_score = float('-inf')\n",
    "        self._best_params = None\n",
    "        self._best_estimator = None\n",
    "        \n",
    "        X_tr, X_v, y_tr, y_v = train_test_split(X, y, test_size=self._cv)\n",
    "        ltf = LoopTimingForecaster(len(self._params_grid))\n",
    "        for params in self._params_grid:\n",
    "            ltf.record_loop_start()\n",
    "            estimator = self._estimator_instantiator().set_params(**params)\n",
    "            estimator.fit(X_tr, y_tr)\n",
    "            score = estimator.score(X_v, y_v)\n",
    "            self._scores.append((params, score))\n",
    "            \n",
    "            if score > self._best_score:\n",
    "                self._best_score = score\n",
    "                self._best_params = params\n",
    "                self._estimator = estimator\n",
    "            else:\n",
    "                del estimator\n",
    "            ltf.record_loop_end()\n",
    "            ltf.report_forecast()\n",
    "                \n",
    "    @property\n",
    "    def best_params_(self):\n",
    "        return self._best_params\n",
    "    \n",
    "    @property\n",
    "    def best_score_(self):\n",
    "        return self._best_score\n",
    "    \n",
    "    @property\n",
    "    def grid_scores_(self):\n",
    "        return self._scores\n",
    "    \n",
    "    @property\n",
    "    def best_estimator_(self):\n",
    "        return self._best_estimator\n",
    "    \n",
    "    def predict(self, X, y):\n",
    "        return self._best_estimator.predict(X, y)\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self._best_estimator.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hidden_units(layer_nums, layer_sizes):\n",
    "    all_hidden_unit_combos = []\n",
    "    for layer_num in layer_nums:\n",
    "        hidden_unit_combos = [combo for combo in itertools.permutations(layer_sizes, layer_num)]\n",
    "        all_hidden_unit_combos.extend(hidden_unit_combos)\n",
    "    return all_hidden_unit_combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layer_sizes = [50, 100, 200, 500]\n",
    "layer_nums = [1, 2, 3, 4]\n",
    "hidden_units = get_hidden_units(layer_nums, layer_sizes)\n",
    "\n",
    "nn_fc_params_grid = {\n",
    " 'batch_size': [100],\n",
    " 'continue_training': [False],\n",
    " 'dropout': [0, 0.1, 0.25, 0.5, 0.7],\n",
    " 'hidden_units': hidden_units,\n",
    " 'learning_rate': [1e-3, 1e-1, 1e-5],\n",
    " 'n_classes': [2],\n",
    " 'optimizer': ['Adam', 'SGD', 'Adagrad'],\n",
    " 'steps': [5000],\n",
    " 'verbose': [0]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting loop timer\n"
     ]
    }
   ],
   "source": [
    "nn_fc_cv = SKFlowGridSearchCV(lambda:skflow.TensorFlowDNNClassifier(hidden_units=[100], n_classes=2), nn_fc_params_grid)\n",
    "nn_fc_cv.fit(X['tr'], pdl.labels['fc']['tr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'continue_training': False, 'optimizer': 'Adam', 'verbose': 1, 'dropout': 0, 'batch_size': 100, 'steps': 5000, 'n_classes': 2, 'hidden_units': (100,), 'learning_rate': 0.001}\n"
     ]
    }
   ],
   "source": [
    "print nn_fc_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = skflow.TensorFlowDNNRegressor(hidden_units=[500, 200], batch_size=batch_size, steps=5000,\n",
    "                                    optimizer=\"Adam\", learning_rate=0.001, dropout=0.25, continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pfc_10'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-939774c02025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpdl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pfc_10'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'pfc_10'"
     ]
    }
   ],
   "source": [
    "reg.fit(pdl.tr, pdl.labels['pfc_10']['tr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression PFC train accuracy 0.775900502456, test accuracy 0.747592332814\n"
     ]
    }
   ],
   "source": [
    "PatchesSKLearner.print_accuracy(reg, pdl.tr, pdl.labels['pfc_10']['tr'], pdl.t, pdl.labels['pfc_10']['t'], \"DNN Regression PFC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression PFC mse train mse 0.0228855780185, test mse 0.0254127871325\n"
     ]
    }
   ],
   "source": [
    "PatchesSKLearner.print_mse(reg, pdl.tr, pdl.labels['pfc_10']['tr'], pdl.t, pdl.labels['pfc_10']['t'], \"DNN Regression PFC mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_ferrari = np.r_[pdl.labels['ferrari']['tr'], pdl.labels['ferrari']['t']]\n",
    "n_ferrari_tr = pdl.labels['ferrari']['tr'].shape[0]\n",
    "ferrari_mean, ferrari_std = np.mean(all_ferrari), np.std(all_ferrari)\n",
    "all_ferrari_normalized = (all_ferrari - ferrari_mean) / ferrari_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ferrari_normalized_tr = all_ferrari_normalized[:n_ferrari_tr]\n",
    "ferrari_normalized_t = all_ferrari_normalized[n_ferrari_tr:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_ferrari = skflow.TensorFlowDNNRegressor(hidden_units=[500, 200], batch_size=batch_size, steps=5000,\n",
    "                                    optimizer=\"Adam\", learning_rate=0.001, dropout=0.25, continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #100, epoch #4, avg. train loss: 0.48840\n",
      "Step #200, epoch #9, avg. train loss: 0.43664\n",
      "Step #300, epoch #13, avg. train loss: 0.41896\n",
      "Step #400, epoch #18, avg. train loss: 0.40423\n",
      "Step #500, epoch #22, avg. train loss: 0.39277\n",
      "Step #600, epoch #27, avg. train loss: 0.38682\n",
      "Step #700, epoch #31, avg. train loss: 0.37682\n",
      "Step #800, epoch #36, avg. train loss: 0.36997\n",
      "Step #900, epoch #40, avg. train loss: 0.36687\n",
      "Step #1000, epoch #45, avg. train loss: 0.35941\n",
      "Step #1100, epoch #50, avg. train loss: 0.35468\n",
      "Step #1200, epoch #54, avg. train loss: 0.34959\n",
      "Step #1300, epoch #59, avg. train loss: 0.35008\n",
      "Step #1400, epoch #63, avg. train loss: 0.34232\n",
      "Step #1500, epoch #68, avg. train loss: 0.33779\n",
      "Step #1600, epoch #72, avg. train loss: 0.32967\n",
      "Step #1700, epoch #77, avg. train loss: 0.33234\n",
      "Step #1800, epoch #81, avg. train loss: 0.32764\n",
      "Step #1900, epoch #86, avg. train loss: 0.32361\n",
      "Step #2000, epoch #90, avg. train loss: 0.32120\n",
      "Step #2100, epoch #95, avg. train loss: 0.31596\n",
      "Step #2200, epoch #100, avg. train loss: 0.31215\n",
      "Step #2300, epoch #104, avg. train loss: 0.30994\n",
      "Step #2400, epoch #109, avg. train loss: 0.30851\n",
      "Step #2500, epoch #113, avg. train loss: 0.30000\n",
      "Step #2600, epoch #118, avg. train loss: 0.29304\n",
      "Step #2700, epoch #122, avg. train loss: 0.29849\n",
      "Step #2800, epoch #127, avg. train loss: 0.29075\n",
      "Step #2900, epoch #131, avg. train loss: 0.28860\n",
      "Step #3000, epoch #136, avg. train loss: 0.27997\n",
      "Step #3100, epoch #140, avg. train loss: 0.27983\n",
      "Step #3200, epoch #145, avg. train loss: 0.27756\n",
      "Step #3300, epoch #150, avg. train loss: 0.27622\n",
      "Step #3400, epoch #154, avg. train loss: 0.26939\n",
      "Step #3500, epoch #159, avg. train loss: 0.27187\n",
      "Step #3600, epoch #163, avg. train loss: 0.26315\n",
      "Step #3700, epoch #168, avg. train loss: 0.26451\n",
      "Step #3800, epoch #172, avg. train loss: 0.25885\n",
      "Step #3900, epoch #177, avg. train loss: 0.25813\n",
      "Step #4000, epoch #181, avg. train loss: 0.25351\n",
      "Step #4100, epoch #186, avg. train loss: 0.25194\n",
      "Step #4200, epoch #190, avg. train loss: 0.25376\n",
      "Step #4300, epoch #195, avg. train loss: 0.24733\n",
      "Step #4400, epoch #200, avg. train loss: 0.24353\n",
      "Step #4500, epoch #204, avg. train loss: 0.24457\n",
      "Step #4600, epoch #209, avg. train loss: 0.23883\n",
      "Step #4700, epoch #213, avg. train loss: 0.23185\n",
      "Step #4800, epoch #218, avg. train loss: 0.23460\n",
      "Step #4900, epoch #222, avg. train loss: 0.22829\n",
      "Step #5000, epoch #227, avg. train loss: 0.22515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNRegressor(batch_size=100, clip_gradients=5.0, config=None,\n",
       "            continue_training=True, dropout=0.25, hidden_units=[500, 200],\n",
       "            learning_rate=0.001, n_classes=0, optimizer='Adam', steps=5000,\n",
       "            verbose=1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ferrari.fit(X['tr'], ferrari_normalized_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression Ferrari Canny Normalized train accuracy 0.611247086652, test accuracy 0.243951896547\n"
     ]
    }
   ],
   "source": [
    "PatchesSKLearner.print_accuracy(reg_ferrari, X['tr'], ferrari_normalized_tr, X['t'], \n",
    "                                ferrari_normalized_t, \"DNN Regression Ferrari Canny Normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression Ferrari Canny Normalized mse train mse 0.388062952178, test mse 0.759942829183\n"
     ]
    }
   ],
   "source": [
    "PatchesSKLearner.print_mse(reg_ferrari, X['tr'], ferrari_normalized_tr, X['t'], \n",
    "                                ferrari_normalized_t, \"DNN Regression Ferrari Canny Normalized mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ferrari_preds_normalized_tr = reg_ferrari.predict(X['tr'])\n",
    "ferrari_preds_normalized_t = reg_ferrari.predict(X['t'])\n",
    "\n",
    "ferrari_preds_tr = (ferrari_preds_normalized_tr * ferrari_std) + ferrari_mean\n",
    "ferrari_preds_t = (ferrari_preds_normalized_t * ferrari_std) + ferrari_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ferrari_preds_tr = (ferrari_preds_normalized_tr * ferrari_std) + ferrari_mean\n",
    "ferrari_preds_t = (ferrari_preds_normalized_t * ferrari_std) + ferrari_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression Ferrari Canny train 0.611247094, test 0.243951889254\n"
     ]
    }
   ],
   "source": [
    "print \"{0} train {1}, test {2}\".format(\"DNN Regression Ferrari Canny\", \n",
    "                                               r2_score(pdl.labels['ferrari']['tr'], ferrari_preds_tr), \n",
    "                                               r2_score(pdl.labels['ferrari']['t'], ferrari_preds_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN Regression Ferrari Canny train mse 3.48378707123e-08, test mse 6.82229278498e-08\n"
     ]
    }
   ],
   "source": [
    "print \"{0} train mse {1}, test mse {2}\".format(\"DNN Regression Ferrari Canny\", \n",
    "                                               mean_squared_error(pdl.labels['ferrari']['tr'], ferrari_preds_tr), \n",
    "                                               mean_squared_error(pdl.labels['ferrari']['t'], ferrari_preds_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
