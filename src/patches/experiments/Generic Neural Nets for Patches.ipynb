{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks for Patches\n",
    "Goals\n",
    "* Use Generic DNN for classification\n",
    "* Use Generic DNN for regression\n",
    "* Use combined CNN architecture for classification\n",
    "* Use combined CNN architecture for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.learn as skflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import PatchesDataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '/home/jacky/ws/patches/data/'\n",
    "pdl = PatchesDataLoader(0.25, data_path, [i for i in range(1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def one_hot(types, labels):\n",
    "    result = np.zeros((labels.shape[0], len(types)))\n",
    "    for i in range(labels.shape[0]):\n",
    "        result[i][types[labels[i]]] = 1\n",
    "    return result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fc_tr_1h = one_hot([0, 1], pdl.labels['fc']['tr'])\n",
    "fc_t_1h = one_hot([0, 1], pdl.labels['fc']['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pfc_10_tr = pdl.labels['pfc_10']['tr'].reshape(-1, 1)\n",
    "pfc_10_t = pdl.labels['pfc_10']['t'].reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(data, i, batch_size):\n",
    "    n = data.shape[0]\n",
    "    start = i*batch_size\n",
    "    end = min(n, (i+1)*batch_size)\n",
    "    return data[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_accuracy(accuracy, tr_data, tr_labels, t_data, t_labels, title, x, y):\n",
    "    tr_accu = accuracy.eval({x: tr_data, y: tr_labels})\n",
    "    t_accu = accuracy.eval({x: t_data, y: t_labels})\n",
    "    print \"{0} training accuracy {1}, test accuracy {2}\".format(title, tr_accu, t_accu)\n",
    "\n",
    "def print_accuracy_sk(predictor, tr_data, tr_labels, t_data, t_labels, title):\n",
    "    tr_accu = predictor.score(tr_data, tr_labels)\n",
    "    t_accu = predictor.score(t_data, t_labels)\n",
    "    \n",
    "    print \"{0} train accuracy {1}, test accuracy {2}\".format(title, tr_accu, t_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(optimizer, cost, accuracy, tr_data, tr_labels, t_data, t_labels, x, y):\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(training_epochs):\n",
    "            avg_cost = 0.\n",
    "            total_batch = int(tr_data.shape[0]/batch_size)\n",
    "            for i in range(total_batch):\n",
    "                batch_x, batch_y = get_batch(tr_data, i, batch_size), get_batch(tr_labels, i, batch_size)\n",
    "                _, c = sess.run([optimizer, cost], feed_dict={x: batch_x, y: batch_y})\n",
    "                avg_cost += c / total_batch\n",
    "            if epoch % display_step == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "        print \"Optimization Finished!\"\n",
    "\n",
    "        print_accuracy(accuracy, tr_data, tr_labels, t_data, t_labels, \"DNN\", x, y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 2\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 500 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_hidden_3 = 256 # 2nd layer number of features\n",
    "n_input = 474\n",
    "n_classes = 2\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input], name='x')\n",
    "y = tf.placeholder(\"float\", [None, n_classes], name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_classifier = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy_clf = tf.reduce_mean(tf.cast(loss_classifier, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 14.644018516\n",
      "Epoch: 0003 cost= 4.101628114\n",
      "Epoch: 0005 cost= 3.021985632\n",
      "Epoch: 0007 cost= 2.194346967\n",
      "Epoch: 0009 cost= 1.881359585\n",
      "Epoch: 0011 cost= 1.527652933\n",
      "Epoch: 0013 cost= 1.514903111\n",
      "Epoch: 0015 cost= 1.228939917\n",
      "Epoch: 0017 cost= 1.283466823\n",
      "Epoch: 0019 cost= 1.055163423\n",
      "Optimization Finished!\n",
      "DNN training accuracy 0.909541845322, test accuracy 0.890934109688\n"
     ]
    }
   ],
   "source": [
    "train(optimizer, cost, accuracy_clf, X_tr, fc_tr_1h, X_t, fc_t_1h, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg = skflow.TensorFlowDNNRegressor(hidden_units=[400, 300, 200], batch_size=batch_size, steps=10000,\n",
    "                                    optimizer=\"Adam\", learning_rate=0.001, dropout=0.25, continue_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step #99, avg. train loss: 0.06888\n",
      "Step #199, avg. train loss: 0.03039\n",
      "Step #299, avg. train loss: 0.02779\n",
      "Step #400, epoch #1, avg. train loss: 0.02666\n",
      "Step #500, epoch #1, avg. train loss: 0.02580\n",
      "Step #600, epoch #1, avg. train loss: 0.02471\n",
      "Step #700, epoch #2, avg. train loss: 0.02399\n",
      "Step #800, epoch #2, avg. train loss: 0.02364\n",
      "Step #900, epoch #2, avg. train loss: 0.02231\n",
      "Step #1000, epoch #2, avg. train loss: 0.02170\n",
      "Step #1100, epoch #3, avg. train loss: 0.02122\n",
      "Step #1200, epoch #3, avg. train loss: 0.02084\n",
      "Step #1300, epoch #3, avg. train loss: 0.02039\n",
      "Step #1400, epoch #4, avg. train loss: 0.02045\n",
      "Step #1500, epoch #4, avg. train loss: 0.02042\n",
      "Step #1600, epoch #4, avg. train loss: 0.01971\n",
      "Step #1700, epoch #4, avg. train loss: 0.01964\n",
      "Step #1800, epoch #5, avg. train loss: 0.01949\n",
      "Step #1900, epoch #5, avg. train loss: 0.01927\n",
      "Step #2000, epoch #5, avg. train loss: 0.01928\n",
      "Step #2100, epoch #6, avg. train loss: 0.01897\n",
      "Step #2200, epoch #6, avg. train loss: 0.01890\n",
      "Step #2300, epoch #6, avg. train loss: 0.01860\n",
      "Step #2400, epoch #6, avg. train loss: 0.01822\n",
      "Step #2500, epoch #7, avg. train loss: 0.01835\n",
      "Step #2600, epoch #7, avg. train loss: 0.01782\n",
      "Step #2700, epoch #7, avg. train loss: 0.01808\n",
      "Step #2800, epoch #8, avg. train loss: 0.01769\n",
      "Step #2900, epoch #8, avg. train loss: 0.01732\n",
      "Step #3000, epoch #8, avg. train loss: 0.01770\n",
      "Step #3100, epoch #8, avg. train loss: 0.01751\n",
      "Step #3200, epoch #9, avg. train loss: 0.01750\n",
      "Step #3300, epoch #9, avg. train loss: 0.01650\n",
      "Step #3400, epoch #9, avg. train loss: 0.01679\n",
      "Step #3500, epoch #10, avg. train loss: 0.01641\n",
      "Step #3600, epoch #10, avg. train loss: 0.01614\n",
      "Step #3700, epoch #10, avg. train loss: 0.01660\n",
      "Step #3800, epoch #10, avg. train loss: 0.01685\n",
      "Step #3900, epoch #11, avg. train loss: 0.01640\n",
      "Step #4000, epoch #11, avg. train loss: 0.01595\n",
      "Step #4100, epoch #11, avg. train loss: 0.01610\n",
      "Step #4200, epoch #12, avg. train loss: 0.01586\n",
      "Step #4300, epoch #12, avg. train loss: 0.01599\n",
      "Step #4400, epoch #12, avg. train loss: 0.01620\n",
      "Step #4500, epoch #12, avg. train loss: 0.01548\n",
      "Step #4600, epoch #13, avg. train loss: 0.01574\n",
      "Step #4700, epoch #13, avg. train loss: 0.01517\n",
      "Step #4800, epoch #13, avg. train loss: 0.01510\n",
      "Step #4900, epoch #14, avg. train loss: 0.01555\n",
      "Step #5000, epoch #14, avg. train loss: 0.01504\n",
      "Step #5100, epoch #14, avg. train loss: 0.01528\n",
      "Step #5200, epoch #14, avg. train loss: 0.01505\n",
      "Step #5300, epoch #15, avg. train loss: 0.01482\n",
      "Step #5400, epoch #15, avg. train loss: 0.01424\n",
      "Step #5500, epoch #15, avg. train loss: 0.01487\n",
      "Step #5600, epoch #16, avg. train loss: 0.01441\n",
      "Step #5700, epoch #16, avg. train loss: 0.01462\n",
      "Step #5800, epoch #16, avg. train loss: 0.01431\n",
      "Step #5900, epoch #16, avg. train loss: 0.01452\n",
      "Step #6000, epoch #17, avg. train loss: 0.01424\n",
      "Step #6100, epoch #17, avg. train loss: 0.01411\n",
      "Step #6200, epoch #17, avg. train loss: 0.01418\n",
      "Step #6300, epoch #18, avg. train loss: 0.01364\n",
      "Step #6400, epoch #18, avg. train loss: 0.01365\n",
      "Step #6500, epoch #18, avg. train loss: 0.01447\n",
      "Step #6600, epoch #18, avg. train loss: 0.01350\n",
      "Step #6700, epoch #19, avg. train loss: 0.01396\n",
      "Step #6800, epoch #19, avg. train loss: 0.01336\n",
      "Step #6900, epoch #19, avg. train loss: 0.01350\n",
      "Step #7000, epoch #20, avg. train loss: 0.01358\n",
      "Step #7100, epoch #20, avg. train loss: 0.01345\n",
      "Step #7200, epoch #20, avg. train loss: 0.01374\n",
      "Step #7300, epoch #20, avg. train loss: 0.01339\n",
      "Step #7400, epoch #21, avg. train loss: 0.01288\n",
      "Step #7500, epoch #21, avg. train loss: 0.01346\n",
      "Step #7600, epoch #21, avg. train loss: 0.01341\n",
      "Step #7700, epoch #22, avg. train loss: 0.01299\n",
      "Step #7800, epoch #22, avg. train loss: 0.01337\n",
      "Step #7900, epoch #22, avg. train loss: 0.01306\n",
      "Step #8000, epoch #22, avg. train loss: 0.01276\n",
      "Step #8100, epoch #23, avg. train loss: 0.01273\n",
      "Step #8200, epoch #23, avg. train loss: 0.01262\n",
      "Step #8300, epoch #23, avg. train loss: 0.01258\n",
      "Step #8400, epoch #24, avg. train loss: 0.01318\n",
      "Step #8500, epoch #24, avg. train loss: 0.01263\n",
      "Step #8600, epoch #24, avg. train loss: 0.01267\n",
      "Step #8700, epoch #24, avg. train loss: 0.01243\n",
      "Step #8800, epoch #25, avg. train loss: 0.01261\n",
      "Step #8900, epoch #25, avg. train loss: 0.01252\n",
      "Step #9000, epoch #25, avg. train loss: 0.01242\n",
      "Step #9100, epoch #26, avg. train loss: 0.01245\n",
      "Step #9200, epoch #26, avg. train loss: 0.01210\n",
      "Step #9300, epoch #26, avg. train loss: 0.01206\n",
      "Step #9400, epoch #26, avg. train loss: 0.01258\n",
      "Step #9500, epoch #27, avg. train loss: 0.01196\n",
      "Step #9600, epoch #27, avg. train loss: 0.01199\n",
      "Step #9700, epoch #27, avg. train loss: 0.01269\n",
      "Step #9800, epoch #28, avg. train loss: 0.01213\n",
      "Step #9900, epoch #28, avg. train loss: 0.01191\n",
      "Step #10000, epoch #28, avg. train loss: 0.01235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorFlowDNNRegressor(batch_size=100, clip_gradients=5.0, config=None,\n",
       "            continue_training=True, dropout=0.25,\n",
       "            hidden_units=[400, 300, 200], learning_rate=0.001, n_classes=0,\n",
       "            optimizer='Adam', steps=10000, verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.fit(X_tr, pfc_10_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNN train accuracy 0.796626172124, test accuracy 0.759615214799\n"
     ]
    }
   ],
   "source": [
    "print_accuracy_sk(reg, X_tr, pfc_10_tr, X_t, pfc_10_t, \"DNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_accuracy_sk(reg, X_tr, fer, X_t, pfc_10_t, \"DNN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
